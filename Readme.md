ðŸŽ‰ AWS Data Lake Pipeline Project - Amazon Dataset ðŸŽ‰
Welcome to the AWS Data Lake Pipeline Project! ðŸš€ This repository documents the end-to-end implementation of a data lake to ingest, catalog, transform, query, and secure an amazon.csv dataset using AWS services. Built from scratch on August 25, 2025, this project showcases a scalable solution for managing product data (e.g., category, rating). Letâ€™s explore the journey! ðŸŒŸ


![AWS Data Lake Architecture](images/Architecture.png)

ðŸŒŸ Overview of the Project
This project constructs a data pipeline to process and analyze an amazon.csv dataset containing product details. We utilized AWS services to:

ðŸ“¥ Ingest and Store: Load raw data into Amazon S3.
ðŸ“š Catalog: Generate metadata with AWS Glue.
ðŸ”§ Transform (ETL): Optimize data with Glue Jobs.
ðŸ“Š Query: Perform SQL queries using Amazon Athena.
ðŸ”’ Secure: Enforce access control with AWS Lake Formation.
ðŸ‘€ Monitor: Track operations with CloudWatch.

The pipeline is anchored around the S3 bucket analysis-data-lake-bucket-20250825, Glue database amazon_db, table amazon_csv, and the IAM role glue-etl-role (ARN: arn:aws:iam::747757438809:role/glue-etl-role). ðŸŽ¯

ðŸš€ Step-by-Step Implementation
Hereâ€™s a detailed log of the steps we executed to build this project:
1. ðŸ“¥ Preparation and Setup (Infrastructure Foundation)

Actions:
Signed into the AWS Console (console.aws.amazon.com) and selected region us-east-1. ðŸŒ
Created S3 bucket analysis-data-lake-bucket-20250825 with:
Block public access enabled. ðŸ”’
Server-side encryption (SSE-S3) activated. ðŸ”
Versioning enabled for data recovery. ðŸ”„
(Optional) Lifecycle rule planned for future optimization. â³
![S3-Buckets](images/S3-Buckets.png)



Created IAM role glue-etl-role with policies:
AWSGlueServiceRole for Glue access. ðŸ› ï¸
Custom S3 policy for analysis-data-lake-bucket-20250825. ðŸ“¦
AmazonAthenaFullAccess for querying. ðŸ“Š
Trust policy updated for glue.amazonaws.com and lakeformation.amazonaws.com. ðŸ”‘



(Optional) Set up VPC endpoints for S3 and Glue for private traffic. ðŸŒ


Outcome: Secure infrastructure foundation established. âœ…
![Glue-Role](images/Glue-Role.png)

2. ðŸ“¥ Data Ingestion (Getting Data into the Lake)

Actions:
Prepared amazon.csv with product data (e.g., category, rating). ðŸ“‹
Uploaded amazon.csv to s3://analysis-data-lake-bucket-20250825/raw/amazon/ via S3 console. ðŸ“¤
Organized S3 structure with raw/, refined/, and athena-results/ folders. ðŸ“‚

Outcome: Raw data ingested and stored durably in S3. âœ…
![Analyis Data Set and S3 Structure](images/Analysis-Report.png)

3. ðŸ“š Cataloging Data (Metadata Management)

Actions:
Created Glue database amazon_db in the Glue console. ðŸ“–
Configured crawler amazon-crawler to scan s3://analysis-data-lake-bucket-20250825/raw/amazon/:
IAM role: glue-etl-role. ðŸ”‘
Target database: amazon_db. ðŸ“š
Ran crawler to generate amazon_csv table with inferred schema. ðŸ•µï¸â€â™‚ï¸


Troubleshot TABLE_NOT_FOUND by re-verifying S3 data and re-running the crawler. ðŸ› ï¸


Outcome: Metadata cataloged, enabling queryability. âœ…
![Crawler](images/Crawler.png)

![DataCatalog Database and Table View](images/Table.png)



4. ðŸ”§ ETL (Extract, Transform, Load)

Actions:
Created Glue job amazon-etl-job with:
IAM role: glue-etl-role. ðŸ”§
Type: Spark (PySpark). âš¡
Workers: 2 DPUs. ðŸ’»
Output path: s3://analysis-data-lake-bucket-20250825/refined/amazon-parquet/. ðŸ“¦


Wrote and executed PySpark script:import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import col

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

datasource = glueContext.create_dynamic_frame.from_catalog(database="amazon_db", table_name="amazon_csv")
transformed = datasource.toDF().na.drop(subset=["rating"]).withColumn("discount_percentage", col("discount_percentage").cast("float"))
aggregated = transformed.groupBy("category").agg({"rating": "avg"}).withColumnRenamed("avg(rating)", "avg_rating")
aggregated.write.mode("overwrite").partitionBy("category").parquet("s3://analysis-data-lake-bucket-20250825/refined/amazon-parquet/")

job.commit()


Ran the job and created a new crawler for refined/amazon-parquet/ to generate amazon_refined. ðŸ”„


Outcome: Transformed data optimized as Parquet with aggregated insights. âœ…

![ETL-JOB](images/etl-job.png)


5. ðŸ“Š Querying Data (Ad-Hoc Analysis)

Actions:
Created Athena workgroup amazon-workgroup via AWS CLI:aws athena create-work-group --name amazon-workgroup --configuration '{"ResultConfiguration": {"OutputLocation": "s3://analysis-data-lake-bucket-20250825/athena-results/"}, "ExecutionRole": "arn:aws:iam::747757438809:role/glue-etl-role", "EnforceWorkGroupConfiguration": true, "PublishCloudWatchMetricsEnabled": false}'


Set query result location to s3://analysis-data-lake-bucket-20250825/athena-results/. ðŸ“
Ran queries:
Raw: SELECT category, rating FROM amazon_csv WHERE rating > 4 ORDER BY rating DESC LIMIT 10;
Refined: SELECT category, avg_rating FROM amazon_refined ORDER BY avg_rating DESC;


Resolved TABLE_NOT_FOUND by re-running the crawler. ðŸ•’


Outcome: Successfully queried raw and refined data. âœ…
![Athena Query](images/AthenaQuery.png)


6. ðŸ”’ Security and Governance (Access Control)

Actions:
Set up Lake Formation:
Granted admin permissions to admin-analysis-team. ðŸ‘¤
Registered s3://analysis-data-lake-bucket-20250825/ with glue-etl-role. ðŸŒ
Enabled Lake Formation Mode. âš™ï¸


Applied permissions:
glue-etl-role: Describe, Alter on amazon_db, Select, Describe on amazon_csv (columns category, rating), and Data access on S3. ðŸ”
Revoked default IAMAllowedPrincipals permissions. ðŸš«




Outcome: Data lake secured with fine-grained access. âœ…
![LakeFormation](images/LakeFormation.png)
![AdminUser](images/AnalysisTeamUser.png)
![Adminstrative Roles and Tasks](images/AdminRolesAndTask.png)
![Colum-Based-Access](images/Column-Based-Access.png)
![Trust-Relationship-Glue-Role](images/Trust-Relationship-Glue-Role.png)


7. ðŸ‘€ Visualization and Insights (Reporting)

Actions:
Set up Amazon QuickSight:
Signed up for a free trial. ðŸ“Š
Granted access to S3 and Athena. ðŸ”‘


Created dataset and dashboard:
Source: amazon_db.amazon_refined.
Visual: Bar chart of category vs. avg_rating for top-rated products. ðŸ“ˆ
Published and shared the dashboard. ðŸŒ




Outcome: Visual insights generated (e.g., top-rated categories). âœ…
![QuickSight DashBoard - 1](images/QuickSight-1.png)
![QuickSight DashBoard - 2 ](images/QuickSight-2.png)

8. ðŸ” Testing, Monitoring, and Optimization

Actions:
Tested: Deleted a file in raw/, restored via versioning, and re-ran crawler/ETL/query. âœ…
Monitored: Set up CloudWatch metrics for Glue job duration and Athena data scanned. ðŸ“¡
Optimized: Used Parquet partitioning to reduce query costs. ðŸ’¡
Cleaned Up: Emptied/deleted S3 bucket, stopped Glue jobs, and deleted QuickSight resources. ðŸ§¹


Outcome: Reliable, optimized, and cost-effective pipeline. âœ…
![CloudWatch Monitoring](images/CloudWatch.png)


ðŸŒˆ Architecture Diagram
[External Data Source] --> [Amazon S3] --> [AWS Glue] --> [AWS Glue Jobs] --> [Amazon Athena] --> [AWS Lake Formation & CloudWatch]
    |                        |                |                    |                     |                        |
    +---- Raw Data --------->|------ Catalog -->|----- Transform ---->|------- Query ------->|----- Secure & Monitor -+
                             (amazon_db, amazon_csv)          (amazon_refined)           (category, rating)       (Permissions, Logs)


S3: Stores analysis-data-lake-bucket-20250825 with raw/, refined/, and athena-results/.
Glue: Manages amazon_db, amazon_csv, and amazon_refined via amazon-crawler and amazon-etl-job.
Athena: Queries with amazon-workgroup.
Lake Formation: Secures with admin-analysis-team and glue-etl-role.
CloudWatch: Monitors logs and metrics.


ðŸŽ¯ Project Outcomes

ðŸ“ˆ Built a fully functional data lake pipeline from scratch.
ðŸ”’ Implemented robust security with Lake Formation.
ðŸ“Š Enabled querying and visualization of key metrics.
ðŸš€ Optimized for scalability and cost efficiency.




